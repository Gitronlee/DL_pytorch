{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gitronlee/DL_pytorch/blob/master/FCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QOEXfWpmKdF",
        "colab_type": "text"
      },
      "source": [
        "#### nn.Module(模组)\n",
        "在pytorch里编写网络，所有的层结构和损失函数都来自于 torch.nn 所有的模型构建都是从这个基类 nn.Module 继承的。于是有下面这个模版："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi13LIu4mtxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net_name(nn.Module):\n",
        "    def __init__(self, other_arguments):\n",
        "        super(net_name, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
        "        # cother network layer\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    # 损失函数\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = criterion(output, target)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svwOwJ92nj56",
        "colab_type": "text"
      },
      "source": [
        "计算图，可复用多次，得益于pytorch的自动求导功能，我们不需要自己编写反响传播。常见的损失函数nn中已有，如均方误差，多分类交叉熵等等。\n",
        "\n",
        "#### totch.optim(优化)\n",
        "优化算法就是一种调整模型参数更新的策略。\n",
        "##### 一阶优化算法：梯度下降（学习率。梯度）\n",
        "##### 二阶优化算法：二阶导数（hessian方法，牛顿法，计算成本高）torch.optim 是一个实现各种优化算法的包。\n",
        "\n",
        "#### 模型保存和加载\n",
        "保存：   \n",
        "1、保存整个模型的结构信息和参数信息 torch.save(model, \"./model.pth\")   \n",
        "2、保存模型的状态 torch.save(model.state_dict(), \"./model_state.pth\")   \n",
        "加载：   \n",
        "1、load_model = torch.load(\"model.pth\")   \n",
        "2、需要先导入模型结构，再导入状态：model.load_state_dic(torch,load(\"model_state.pth\"))\n"
      ]
    }
  ]
}